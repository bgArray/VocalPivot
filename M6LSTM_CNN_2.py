import os
import numpy as np
import numpy.typing as npt  # å¯¼å…¥NumPyç±»å‹æ³¨è§£æ¨¡å—
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, Masking, Conv1D, BatchNormalization  # , MaxPooling1D
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from keras import optimizers, losses
from tensorflow import reduce_sum, maximum


# -------------------------- 1. æ•°æ®åŠ è½½å‡½æ•°ï¼ˆä¿ç•™æ ¸å¿ƒé€»è¾‘ï¼Œä¼˜åŒ–æ—¥å¿—è¾“å‡ºï¼‰ --------------------------
def load_data(file_path):
    """åŠ è½½.npyæ•°æ®ï¼Œæå–0/1/2æ ‡ç­¾å¹¶ç¡®ä¿ç‰¹å¾ä¸æ ‡ç­¾é•¿åº¦ä¸€è‡´"""
    data = []
    # ç»Ÿè®¡æœ‰æ•ˆ/æ— æ•ˆæ ·æœ¬æ•°ï¼Œä¾¿äºåç»­åˆ†æ
    valid_count = 0
    invalid_count = 0

    for filename in os.listdir(file_path):
        if filename.endswith('.npy'):
            file_full_path = os.path.join(file_path, filename)
            try:
                # æ•è·å¼‚å¸¸ï¼Œé¿å…å•ä¸ªæŸåæ–‡ä»¶å¯¼è‡´ç¨‹åºå´©æºƒ
                batch_data = np.load(file_full_path, allow_pickle=True)
                data.extend(batch_data)
                valid_count += len(batch_data)
            except Exception as e:
                print(f"è­¦å‘Šï¼šæ–‡ä»¶{filename}åŠ è½½å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
                invalid_count += 1

    X = []  # å­˜å‚¨melç‰¹å¾åºåˆ—
    y = []  # å­˜å‚¨æ ‡ç­¾åºåˆ—
    mismatch_count = 0  # ç»Ÿè®¡é•¿åº¦ä¸åŒ¹é…æ ·æœ¬

    for item in data:
        mel_spectrogram, tags = item
        # ç¡®ä¿melå’Œæ ‡ç­¾æ—¶é—´æ­¥ä¸€è‡´
        if len(mel_spectrogram) != len(tags):
            mismatch_count += 1
            continue
        # è¿‡æ»¤æ— æ•ˆæ ‡ç­¾ï¼Œä»…ä¿ç•™0/1/2ï¼Œæ— æœ‰æ•ˆæ ‡ç­¾æ ‡è®°ä¸º-1
        valid_tags = []
        for tag_group in tags:
            filtered = [t for t in tag_group if t in {"0", "1", "2"}]
            valid_tags.append(filtered[0] if filtered else "-1")
        X.append(mel_spectrogram)
        y.append(valid_tags)

    # è¾“å‡ºæ•°æ®åŠ è½½ç»Ÿè®¡ä¿¡æ¯
    print(f"æ•°æ®åŠ è½½å®Œæˆï¼š")
    print(f"- æˆåŠŸåŠ è½½æ–‡ä»¶æ•°ï¼š{len(os.listdir(file_path)) - invalid_count}")
    print(f"- æ€»æ ·æœ¬æ•°ï¼ˆå«æ— æ•ˆï¼‰ï¼š{valid_count}")
    print(f"- é•¿åº¦ä¸åŒ¹é…æ ·æœ¬æ•°ï¼š{mismatch_count}")
    print(f"- æœ€ç»ˆæœ‰æ•ˆæ ·æœ¬æ•°ï¼š{len(X)}")
    return X, y


# -------------------------- 2. æ•°æ®å¢å¼ºå‡½æ•°ï¼ˆä»…ä¿ç•™è½»é‡çº§é¢‘è°±å¢å¼ºï¼Œç§»é™¤æ—¶åºè£å‰ªï¼‰ --------------------------
def augment_mel_spectrogram(mel_seq):
    """è½»é‡çº§é¢‘è°±å¢å¼ºï¼šä»…ä¿ç•™é¢‘åŸŸåç§»+å¹…åº¦ç¼©æ”¾ï¼Œé™ä½å™ªå£°å¹²æ‰°"""
    aug_seq = mel_seq.copy()
    # å›ºå®šéšæœºç§å­ç¡®ä¿å¢å¼ºç¨³å®šæ€§ï¼ˆé¿å…æ¯æ¬¡è¿è¡Œå·®å¼‚è¿‡å¤§ï¼‰
    np.random.seed(42)

    # 1. å¾®å°é¢‘åŸŸåç§»ï¼ˆÂ±1ä¸ªæ¢…å°”ç³»æ•°ï¼Œå‡å°‘å…³é”®é¢‘å¸¦ç ´åï¼‰
    shift = np.random.randint(-1, 2)  # åç§»èŒƒå›´ä»Â±2â†’Â±1
    if shift != 0:
        aug_seq = np.roll(aug_seq, shift, axis=1)
        # ç©ºç™½å¤„å¡«å……åŸåºåˆ—è¾¹ç¼˜å‡å€¼ï¼Œé¿å…ç¡¬å¡«å……0å¯¼è‡´ç‰¹å¾çªå˜
        edge_mean = aug_seq[:, shift:shift + 1].mean() if shift > 0 else aug_seq[:, shift - 1:shift].mean()
        if shift > 0:
            aug_seq[:, :shift] = edge_mean
        else:
            aug_seq[:, shift:] = edge_mean

    # 2. æ¸©å’Œå¹…åº¦ç¼©æ”¾ï¼ˆ0.9-1.1å€ï¼Œå‡å°‘åŠ›åº¦å˜åŒ–å¹²æ‰°ï¼‰
    scale = np.random.uniform(0.9, 1.1)  # ç¼©æ”¾èŒƒå›´ä»0.8-1.2â†’0.9-1.1
    aug_seq = aug_seq * scale

    return aug_seq


# -------------------------- 3. æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼ˆå‰Šå‡å¢å¼ºæ¯”ä¾‹ï¼Œä¼˜åŒ–æ•°æ®åˆ†å¸ƒï¼‰ --------------------------
def preprocess_data(
    X: list[npt.NDArray[np.float32]],  # æ˜ç¡®Xæ˜¯float32æ•°ç»„çš„åˆ—è¡¨
    y: list[list[str]],  # æ˜ç¡®yæ˜¯å­—ç¬¦ä¸²æ ‡ç­¾åˆ—è¡¨çš„åˆ—è¡¨
    le: LabelEncoder | None = None,
    max_length: int | None = None,
    is_train: bool = True
) -> tuple[npt.NDArray[np.float32], npt.NDArray[np.float32], int, LabelEncoder]:
    # å‡½æ•°é€»è¾‘ä¸å˜...
    """
    ä¼˜åŒ–åé¢„å¤„ç†ï¼šè½»é‡çº§å¢å¼º+åºåˆ—å¡«å……+æ ‡ç­¾ç¼–ç 
    å…³é”®ä¿®å¤ï¼šnumpy.pad()å‚æ•°æ ¼å¼ä¸ç±»å‹é”™è¯¯
    """
    # 1. æ¢…å°”é¢‘è°±é¢‘åŸŸåˆ‡ç‰‡ï¼šä¿ç•™200Hz-5kHzå…³é”®é¢‘å¸¦ï¼ˆåŸ128ç»´â†’74ç»´ï¼‰
    X_sliced = []
    target_dim = 74  # ç›®æ ‡é¢‘åŸŸç»´åº¦ï¼Œæ˜ç¡®å˜é‡é¿å…é­”æ³•æ•°å­—
    for mel_seq in X:
        # ç¡®ä¿mel_seqæ˜¯numpyæ•°ç»„ï¼ˆé¿å…è¾“å…¥ç±»å‹å¼‚å¸¸ï¼‰
        mel_seq = np.asarray(mel_seq, dtype=np.float32)
        current_dim = mel_seq.shape[1]  # å½“å‰é¢‘åŸŸç»´åº¦

        if current_dim >= target_dim:
            # ç»´åº¦è¶³å¤Ÿï¼šç›´æ¥åˆ‡ç‰‡å–å‰74ç»´
            sliced = mel_seq[:, 0:target_dim]
        else:
            # ç»´åº¦ä¸è¶³ï¼šè®¡ç®—éœ€è¦å¡«å……çš„é•¿åº¦ï¼ˆç¡®ä¿æ˜¯æ•´æ•°ï¼‰
            pad_width = target_dim - current_dim
            # ä¿®å¤pad_widthæ ¼å¼ï¼šå¿…é¡»æ˜¯"(è½´1å¡«å……, è½´2å¡«å……)"ï¼Œä¸”å¡«å……é•¿åº¦ä¸ºéè´Ÿæ•´æ•°
            sliced = np.pad(
                mel_seq,
                pad_width=((0, 0), (0, pad_width)),  # è½´0ï¼ˆæ—¶é—´æ­¥ï¼‰ä¸å¡«å……ï¼Œè½´1ï¼ˆé¢‘åŸŸï¼‰åå¡«å……
                mode='constant',  # å¡«å……æ¨¡å¼ï¼šå¸¸æ•°å¡«å……
                constant_values=0.0  # å¡«å……å€¼ï¼š0.0ï¼ˆæ˜ç¡®æŒ‡å®šï¼Œé¿å…é»˜è®¤å€¼ç±»å‹é—®é¢˜ï¼‰
            )
        X_sliced.append(sliced)
    X = X_sliced

    # 2. è®­ç»ƒé›†è½»é‡çº§å¢å¼ºï¼ˆä»…50%æ ·æœ¬å¢å¼ºï¼Œé¿å…å™ªå£°æ³›æ»¥ï¼‰
    if is_train:
        X_augmented = []
        y_augmented = []
        for mel_seq, tag_seq in zip(X, y):
            # ä¿ç•™åŸå§‹æ ·æœ¬
            X_augmented.append(mel_seq)
            y_augmented.append(tag_seq)
            # 50%æ¦‚ç‡ç”Ÿæˆå¢å¼ºæ ·æœ¬ï¼Œå¹³è¡¡æ•°æ®é‡ä¸å™ªå£°
            if np.random.random() < 0.5:
                aug_mel = augment_mel_spectrogram(mel_seq)
                X_augmented.append(aug_mel)
                y_augmented.append(tag_seq.copy())
        X, y = X_augmented, y_augmented
        print(f"å¢å¼ºåè®­ç»ƒé›†è§„æ¨¡ï¼š{len(X)}ï¼ˆåŸå§‹æ ·æœ¬+50%å¢å¼ºæ ·æœ¬ï¼‰")

    # 3. æ ‡ç­¾ç¼–ç ï¼ˆè®­ç»ƒé›†æ‹Ÿåˆï¼Œæµ‹è¯•é›†å¤ç”¨ï¼‰
    if is_train:
        # æ‰å¹³åŒ–æ ‡ç­¾å¹¶ç»Ÿè®¡åˆ†å¸ƒï¼Œä¾¿äºåˆ†ææ•°æ®å¹³è¡¡æ€§
        all_tags = [tag for seq in y for tag in seq]
        tag_count = {tag: all_tags.count(tag) for tag in set(all_tags)}
        print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒï¼š{tag_count}")
        # æ‹Ÿåˆæ ‡ç­¾ç¼–ç å™¨
        le = LabelEncoder()
        le.fit(all_tags)

    # 4. æ ‡ç­¾åºåˆ—ç¼–ç ä¸å¡«å……
    y_encoded = [le.transform(seq) for seq in y]
    # ç¡®å®šæœ€å¤§é•¿åº¦ï¼ˆè®­ç»ƒé›†è®¡ç®—ï¼Œæµ‹è¯•é›†å¤ç”¨ï¼‰
    if is_train:
        max_length = max(len(seq) for seq in X)
        print(f"åºåˆ—æœ€å¤§é•¿åº¦ï¼š{max_length}ï¼ˆç”¨äºåç»­å¡«å……ï¼‰")

    # 5. ç‰¹å¾ä¸æ ‡ç­¾å¡«å……ï¼ˆåå¡«å……0ï¼Œä¿æŒæ—¶åºå®Œæ•´æ€§ï¼‰
    # ç‰¹å¾å¡«å……ï¼ˆfloat32ç±»å‹ï¼Œé€‚é…æ¨¡å‹è¾“å…¥ï¼‰
    X_padded = sequence.pad_sequences(
        X, maxlen=max_length, dtype='float32', padding='post', truncating='post', value=0.0
    )
    # æ ‡ç­¾å¡«å……ï¼ˆå¡«å……-1çš„ç¼–ç ï¼Œåç»­é€šè¿‡æ©ç å¿½ç•¥ï¼‰
    no_label_code = le.transform(["-1"])[0]
    y_padded = sequence.pad_sequences(
        y_encoded, maxlen=max_length, padding='post', truncating='post', value=no_label_code
    )

    # 6. æ ‡ç­¾ç‹¬çƒ­ç¼–ç ï¼ˆé€‚é…å¤šåˆ†ç±»ä»»åŠ¡ï¼‰
    num_classes = len(le.classes_)
    y_onehot = to_categorical(y_padded, num_classes=num_classes)
    print(f"é¢„å¤„ç†å®Œæˆï¼šç‰¹å¾å½¢çŠ¶{X_padded.shape}ï¼Œæ ‡ç­¾å½¢çŠ¶{y_onehot.shape}ï¼Œç±»åˆ«æ•°{num_classes}")

    return X_padded, y_onehot, max_length, le


# -------------------------- 4. è‡ªå®šä¹‰æ©ç æŸå¤±å‡½æ•°ï¼ˆå¿½ç•¥æ— æ ‡ç­¾å¸§ï¼Œä¼˜åŒ–æ•°å€¼ç¨³å®šæ€§ï¼‰ --------------------------
def ignore_no_label_loss(no_label_idx):
    """
    é—­åŒ…å®ç°æŸå¤±å‡½æ•°ï¼šä»…è®¡ç®—æœ‰æ•ˆæ ‡ç­¾ï¼ˆ0/1/2ï¼‰çš„æŸå¤±ï¼Œå¿½ç•¥æ— æ ‡ç­¾å¸§ï¼ˆ-1ï¼‰
    è¾“å…¥ï¼šno_label_idx - æ— æ ‡ç­¾å¸§ï¼ˆ-1ï¼‰çš„ç‹¬çƒ­ç¼–ç ç´¢å¼•
    """

    def loss(y_true, y_pred):
        # ç”Ÿæˆæ©ç ï¼šæœ‰æ•ˆå¸§ï¼ˆé-1ï¼‰ä¸º1ï¼Œæ— æ ‡ç­¾å¸§ä¸º0
        mask = 1 - y_true[:, :, no_label_idx]
        # è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼ˆé¿å…æ•°å€¼æº¢å‡ºï¼Œä½¿ç”¨æ•°å€¼ç¨³å®šç‰ˆï¼‰
        ce_loss = losses.categorical_crossentropy(y_true, y_pred, from_logits=False)
        # åº”ç”¨æ©ç ï¼Œä»…ä¼˜åŒ–æœ‰æ•ˆå¸§
        masked_loss = ce_loss * mask
        # æœ‰æ•ˆå¸§æ•°é‡å½’ä¸€åŒ–ï¼ˆæ·»åŠ 1e-8é¿å…é™¤ä»¥0ï¼Œæå‡æ•°å€¼ç¨³å®šæ€§ï¼‰
        valid_frame_count = maximum(reduce_sum(mask), 1e-8)
        return reduce_sum(masked_loss) / valid_frame_count

    return loss


# -------------------------- 5. ç®€åŒ–ç‰ˆCNN-LSTMæ¨¡å‹ï¼ˆé™ä½å¤æ‚åº¦ï¼ŒæŠ‘åˆ¶è¿‡æ‹Ÿåˆï¼‰ --------------------------
# def build_cnn_lstm_model(input_shape, num_classes, no_label_idx):
#     """
#     ä¼˜åŒ–åæ¨¡å‹ç»“æ„ï¼šè½»é‡çº§CNNæå–é¢‘åŸŸç‰¹å¾ + ç²¾ç®€LSTMæ•æ‰æ—¶åºå…³è”
#     å…³é”®ä¿®æ”¹ï¼šå‡å°‘å‚æ•°é‡ã€å¢åŠ æ­£åˆ™åŒ–ã€æ¢å¤æ± åŒ–å±‚
#     """
#     model = Sequential([
#         # 1. 1D-CNNå±‚ï¼šæå–å±€éƒ¨é¢‘åŸŸç‰¹å¾ï¼ˆ32æ»¤æ³¢å™¨ï¼Œæ ¸å¤§å°5ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼‰
#         Conv1D(
#             filters=32,
#             kernel_size=5,
#             activation='relu',
#             input_shape=input_shape,
#             padding='same'  # ä¿æŒæ—¶é—´æ­¥ä¸€è‡´
#         ),
#         BatchNormalization(),  # æ‰¹é‡å½’ä¸€åŒ–ï¼ŒåŠ é€Ÿæ”¶æ•›
#         MaxPooling1D(pool_size=2, padding='same'),  # æ± åŒ–å‹ç¼©æ—¶é—´æ­¥ï¼Œä¿ç•™å…³é”®ç‰¹å¾
#
#         # 2. Maskingå±‚ï¼šè¿‡æ»¤å¡«å……çš„0å€¼ï¼Œé¿å…å¹²æ‰°è®­ç»ƒ
#         Masking(mask_value=0.0),
#
#         # 3. LSTMå±‚ï¼šç²¾ç®€å•å…ƒæ•°ï¼ˆ128â†’64ï¼‰ï¼Œæå‡æ­£åˆ™åŒ–åŠ›åº¦
#         LSTM(
#             units=64,
#             return_sequences=True,  # é€å¸§è¾“å‡ºï¼Œé€‚é…æ—¶åºåˆ†ç±»
#             dropout=0.4,  # è¾“å…¥ dropout æå‡æ³›åŒ–æ€§
#             recurrent_dropout=0.3  # å¾ªç¯ dropout æŠ‘åˆ¶æ—¶åºè¿‡æ‹Ÿåˆ
#         ),
#
#         # 4. å…¨è¿æ¥å±‚ï¼šç²¾ç®€å•å…ƒæ•°ï¼ˆ64â†’32ï¼‰ï¼ŒåŠ å¼ºæ­£åˆ™åŒ–
#         Dense(32, activation='relu'),
#         Dropout(0.5),  # æé«˜dropoutæ¯”ä¾‹ï¼ŒæŠ‘åˆ¶å…¨è¿æ¥å±‚è¿‡æ‹Ÿåˆ
#
#         # 5. è¾“å‡ºå±‚ï¼šå¤šåˆ†ç±»softmaxï¼Œè¾“å‡ºæ¯å¸§ç±»åˆ«æ¦‚ç‡
#         Dense(num_classes, activation='softmax')
#     ])
#
#     # ç¼–è¯‘æ¨¡å‹ï¼šä¼˜åŒ–å™¨+è‡ªå®šä¹‰æ©ç æŸå¤±
#     model.compile(
#         optimizer=optimizers.Adam(learning_rate=0.0008),  # é™ä½åˆå§‹å­¦ä¹ ç‡ï¼Œé¿å…éœ‡è¡
#         loss=ignore_no_label_loss(no_label_idx),  # ä¼ å…¥æ— æ ‡ç­¾å¸§ç´¢å¼•
#         metrics=['accuracy']  # ç›‘æ§å‡†ç¡®ç‡ï¼Œç›´è§‚è¯„ä¼°æ¨¡å‹æ€§èƒ½
#     )
#
#     # æ‰“å°æ¨¡å‹ç»“æ„ä¸å‚æ•°é‡ï¼ˆä¾¿äºç¡®è®¤å¤æ‚åº¦ï¼‰
#     model.summary()
#     return model
def build_cnn_lstm_model(input_shape, num_classes, no_label_idx):
    model = Sequential([
        # 1. 1D-CNNå±‚ï¼šæå–é¢‘åŸŸç‰¹å¾ï¼Œpadding='same'ä¿æŒæ—¶é—´æ­¥ä¸å˜
        Conv1D(
            filters=32,
            kernel_size=5,
            activation='relu',
            input_shape=input_shape,
            padding='same'  # å…³é”®ï¼šç¡®ä¿æ—¶é—´æ­¥ä¸è¢«CNNæ”¹å˜
        ),
        BatchNormalization(),  # ä¿ç•™æ‰¹é‡å½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒ
        # ğŸ‘‡ åˆ é™¤è¿™è¡Œ MaxPooling1Dï¼Œé¿å…æ—¶é—´æ­¥å‹ç¼©
        # MaxPooling1D(pool_size=2, padding='same'),

        # 2. Maskingå±‚ï¼šè¿‡æ»¤å¡«å……çš„0å€¼
        Masking(mask_value=0.0),

        # 3. LSTMå±‚ï¼šä¿æŒreturn_sequences=Trueï¼Œé€å¸§è¾“å‡ºï¼ˆæ—¶é—´æ­¥ä¸è¾“å…¥ä¸€è‡´ï¼‰
        LSTM(
            units=64,
            return_sequences=True,
            dropout=0.4,
            recurrent_dropout=0.3
        ),

        # 4. å…¨è¿æ¥å±‚ä¸è¾“å‡ºå±‚
        Dense(32, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')  # è¾“å‡ºæ—¶é—´æ­¥=894ï¼Œä¸æ ‡ç­¾ä¸€è‡´
    ])

    model.compile(
        optimizer=optimizers.Adam(learning_rate=0.0008),
        loss=ignore_no_label_loss(no_label_idx),
        metrics=['accuracy']
    )
    model.summary()  # ç¼–è¯‘å‰æ‰“å°ç»“æ„ï¼Œç¡®è®¤è¾“å‡ºå½¢çŠ¶ä¸º(None, 894, 4)
    return model

# -------------------------- 6. ä¸»å‡½æ•°ï¼ˆæ•´åˆæ•°æ®æµç¨‹ä¸è®­ç»ƒé€»è¾‘ï¼Œä¼˜åŒ–å‚æ•°ï¼‰ --------------------------
def main(data_path):
    # 1. åŠ è½½åŸå§‹æ•°æ®
    print("=" * 50)
    print("1. å¼€å§‹åŠ è½½æ•°æ®...")
    X_raw, y_raw = load_data(data_path)
    if len(X_raw) == 0:
        print("é”™è¯¯ï¼šæœªåŠ è½½åˆ°æœ‰æ•ˆæ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„ä¸æ–‡ä»¶æ ¼å¼ï¼")
        return

    # 2. åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†ï¼ˆ8:2ï¼Œå›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°ï¼‰
    print("\n" + "=" * 50)
    print("2. åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†...")
    X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
        X_raw, y_raw, test_size=0.2, random_state=42, shuffle=True
    )
    print(f"è®­ç»ƒé›†åŸå§‹æ ·æœ¬æ•°ï¼š{len(X_train_raw)}ï¼Œæµ‹è¯•é›†åŸå§‹æ ·æœ¬æ•°ï¼š{len(X_test_raw)}")

    # 3. æ•°æ®é¢„å¤„ç†ï¼ˆè®­ç»ƒé›†å•ç‹¬å¤„ç†ï¼Œæµ‹è¯•é›†å¤ç”¨å‚æ•°ï¼‰
    print("\n" + "=" * 50)
    print("3. é¢„å¤„ç†è®­ç»ƒé›†...")
    X_train, y_train, max_length, le = preprocess_data(
        X_train_raw, y_train_raw, is_train=True
    )

    print("\n" + "=" * 50)
    print("3. é¢„å¤„ç†æµ‹è¯•é›†...")
    X_test, y_test, _, _ = preprocess_data(
        X_test_raw, y_test_raw, le=le, max_length=max_length, is_train=False
    )

    # 4. è®¡ç®—æ— æ ‡ç­¾å¸§ç´¢å¼•ï¼ˆç”¨äºæŸå¤±å‡½æ•°ï¼‰
    no_label_idx = np.where(le.classes_ == "-1")[0][0]
    print(f"\næ— æ ‡ç­¾å¸§ï¼ˆ-1ï¼‰çš„ç‹¬çƒ­ç¼–ç ç´¢å¼•ï¼š{no_label_idx}")

    # 5. æ„å»ºæ¨¡å‹
    print("\n" + "=" * 50)
    print("4. æ„å»ºCNN-LSTMæ¨¡å‹...")
    input_shape = (max_length, X_train.shape[2])  # (æ—¶é—´æ­¥, é¢‘åŸŸç»´åº¦)
    num_classes = y_train.shape[-1]
    model = build_cnn_lstm_model(input_shape, num_classes, no_label_idx)

    # 6. å®šä¹‰è®­ç»ƒå›è°ƒï¼ˆä¼˜åŒ–æ—©åœã€æ¨¡å‹ä¿å­˜ã€å­¦ä¹ ç‡è°ƒåº¦ï¼‰
    print("\n" + "=" * 50)
    print("5. é…ç½®è®­ç»ƒå›è°ƒ...")
    callbacks = [
        # æ—©åœï¼šç›‘æ§éªŒè¯å‡†ç¡®ç‡ï¼Œè€å¿ƒ3è½®ï¼Œæ¢å¤æœ€ä¼˜æƒé‡ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
        EarlyStopping(
            monitor='val_accuracy',
            patience=3,
            restore_best_weights=True,
            verbose=1,
            mode='max'  # å‡†ç¡®ç‡éœ€æœ€å¤§åŒ–
        ),
        # æ¨¡å‹ä¿å­˜ï¼šä»…ä¿å­˜éªŒè¯å‡†ç¡®ç‡æœ€ä¼˜çš„æ¨¡å‹ï¼ˆç§»é™¤å¤šä½™çš„save_formatå‚æ•°ï¼‰
        ModelCheckpoint(
            'best_cnn_lstm_model.keras',
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
            # æ­¤å¤„åˆ é™¤ save_format='keras'ï¼Œå› Keras 2.15.0ä¸æ”¯æŒè¯¥å‚æ•°
        ),
        # å­¦ä¹ ç‡è°ƒåº¦ï¼šéªŒè¯æŸå¤±3è½®ä¸é™åˆ™é™ç‡ï¼Œé¿å…éœ‡è¡
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.4,  # é™ç‡æ¯”ä¾‹ï¼š0.4â†’åŸå­¦ä¹ ç‡*0.4
            patience=2,
            min_lr=1e-7,  # æœ€ä½å­¦ä¹ ç‡ï¼Œé¿å…åœæ»
            verbose=1
        )
    ]

    # 7. æ¨¡å‹è®­ç»ƒï¼ˆä¼˜åŒ–batch_sizeä¸epochsï¼Œé€‚é…CPUï¼‰
    print("\n" + "=" * 50)
    print("6. å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    history = model.fit(
        X_train, y_train,
        epochs=30,  # å‡å°‘epochsï¼Œé¿å…è¿‡åº¦è®­ç»ƒ
        batch_size=12,  # å°batchæå‡æ³›åŒ–æ€§ï¼Œé€‚é…CPUå†…å­˜
        validation_split=0.1,  # è®­ç»ƒé›†10%ä½œä¸ºéªŒè¯é›†ï¼Œç›‘æ§è¿‡æ‹Ÿåˆ
        shuffle=True,  # æ¯è½®æ‰“ä¹±æ•°æ®ï¼Œæå‡æ³›åŒ–æ€§
        callbacks=callbacks,
        verbose=1  # æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œä¾¿äºå®æ—¶ç›‘æ§
    )

    # 8. æ¨¡å‹è¯„ä¼°ï¼ˆæµ‹è¯•é›†éªŒè¯æ³›åŒ–èƒ½åŠ›ï¼‰
    print("\n" + "=" * 50)
    print("7. æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹...")
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)
    print(f"æµ‹è¯•é›†æœ€ç»ˆç»“æœï¼šå‡†ç¡®ç‡={test_acc:.4f}ï¼ŒæŸå¤±={test_loss:.4f}")

    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹ä¸æ ‡ç­¾ç¼–ç å™¨ï¼ˆä¾¿äºåç»­æ¨ç†ï¼‰
    print("\n" + "=" * 50)
    print("8. ä¿å­˜æ¨¡å‹ä¸ç¼–ç å™¨...")
    model.save('final_cnn_lstm_model.keras')
    np.save('label_encoder.npy', le.classes_)
    print("ä¿å­˜å®Œæˆï¼š")
    print("- æœ€ç»ˆæ¨¡å‹ï¼šfinal_cnn_lstm_model.keras")
    print("- æ ‡ç­¾ç¼–ç å™¨ï¼šlabel_encoder.npy")
    print("=" * 50)


# -------------------------- 7. ç¨‹åºå…¥å£ï¼ˆé€‚é…è·¯å¾„å¯¼å…¥ï¼Œå¢åŠ é”™è¯¯å¤„ç†ï¼‰ --------------------------
if __name__ == "__main__":
    # åŠ è½½æ•°æ®é›†è·¯å¾„ï¼ˆä¼˜å…ˆä»Constant.pyå¯¼å…¥ï¼Œæ— åˆ™æ‰‹åŠ¨è¾“å…¥ï¼‰
    try:
        from Constant import DATASET_PATH

        print(f"ä»Constant.pyåŠ è½½æ•°æ®é›†è·¯å¾„ï¼š{DATASET_PATH}")
    except ImportError:
        print("æœªæ‰¾åˆ°Constant.pyï¼Œéœ€æ‰‹åŠ¨è¾“å…¥æ•°æ®é›†è·¯å¾„ï¼")
        DATASET_PATH = input("è¯·è¾“å…¥.npyæ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆä¾‹ï¼šD:/vocal_datasetï¼‰ï¼š").strip()

    # è·¯å¾„æœ‰æ•ˆæ€§æ£€æŸ¥
    if not os.path.exists(DATASET_PATH):
        print(f"é”™è¯¯ï¼šè·¯å¾„{DATASET_PATH}ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼")
    elif not any(f.endswith('.npy') for f in os.listdir(DATASET_PATH)):
        print(f"é”™è¯¯ï¼šè·¯å¾„{DATASET_PATH}ä¸‹æ— .npyæ–‡ä»¶ï¼Œè¯·ç¡®è®¤æ•°æ®é›†æ ¼å¼ï¼")
    else:
        # å¯åŠ¨ä¸»æµç¨‹
        main(DATASET_PATH)
